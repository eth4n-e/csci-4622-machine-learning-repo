{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 1 - K-Nearest Neighbor Classification and Regression\n",
    "## CSCI 4622 - Fall 2025\n",
    "\n",
    "For today's assignment, we will be implementing our own K-Nearest Neighbors classifier and regressor algorithms.\n",
    "\n",
    "*But Professor Quigley, hasn't someone else already written KNN before?*\n",
    "\n",
    "Yes, you are not the first to implement KNN, or basically any algorithm we'll work with in this class. But\n",
    "1. I'll know that you know what's really going on\n",
    "2. You'll know you can do it, because\n",
    "    1. someday you might have to implement some machine learning algorithm from scratch - maybe for a new platform (do you need to run python on your SmartToaster just to get it to learn how users like their toast?), maybe because you want to tweak the algorithm (there's always a better approach...),\n",
    "    2. maybe because you're working on something important, and you need to control exactly what's on there (should you really be running anaconda on your secret spy plane?).\n",
    "\n",
    "That said - we're not going to implement *everything*. We'll start by importing a few helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.neighbors\n",
    "import data\n",
    "import helpers\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Wait a minute - didn't we just import Scikit-learn (sklearn)? The package with baked-in machine learning tools?*\n",
    " Yes - but it also has a ton of helper functions that we'll be using later.\n",
    "\n",
    "You will be guided through the different questions. You'll be expected to complete the classes and the functions following the provided signatures.\n",
    "\n",
    "Remember to avoid adding positional arguments and make sure the returned values have the correct format (this applies to all assignments).\n",
    "The alternative is that your solution might be rejected by the auto-grader (we won't be debugging your code during grading).\n",
    "We will provide some basic sanity checks. They're in no means exhaustive and passing them does not imply that your solution is 100% correct.\n",
    "\n",
    "For example, you're required to complete the method `compute_something` of class A.\n",
    "We provide examples of acceptable solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell can be removed from the submitted notebook\n",
    "class A:\n",
    "    def compute_something(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns: numpy array of zeros, with shape (4,)\n",
    "        \"\"\"\n",
    "        # BEGIN\n",
    "        answer = None\n",
    "        # END\n",
    "        return answer\n",
    "\n",
    "\n",
    "class A1:  # Acceptable solution:\n",
    "    # - the added y is an optional argument and omitting it does not affect the solution\n",
    "    # - the returned object has the expected structure.\n",
    "    def compute_something(self, y=None):\n",
    "        # BEGIN\n",
    "        return np.zeros((4,))\n",
    "        # END\n",
    "\n",
    "\n",
    "class A2:  # Wrong format:\n",
    "    # - your solution requires a new positional argument y!: critical\n",
    "    # - the returned object does not have the expected format!: critical\n",
    "    # - solution outside the delimiters # BEGIN # END: makes grading easier\n",
    "    def compute_something(self, y):\n",
    "        return [0, 0, 0, 0]\n",
    "\n",
    "\n",
    "class A3:  # Acceptable solution:\n",
    "    # You're free to add your own helper functions/methods,\n",
    "    # but they should be defined in the jupyter notebook (no external files)\n",
    "    def compute_something(self):\n",
    "        # BEGIN\n",
    "        return self.get_zeros(4)\n",
    "        # END\n",
    "\n",
    "    def get_zeros(self, i):\n",
    "        return np.zeros((i,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, let's also load a dataset to play with and start working to build out our own classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "binary_data = data.BinaryData()\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.set_figheight(6), fig.set_figwidth(18)\n",
    "for i, name in enumerate([\"training\", \"validation\", \"test\"]):\n",
    "    axs[i].plot(*binary_data.boundary())\n",
    "    axs[i].set_title(\"%s samples\" % name)\n",
    "axs[0].scatter(binary_data.X_train[:, 0], binary_data.X_train[:, 1], c=binary_data.y_train)\n",
    "axs[1].scatter(binary_data.X_valid[:, 0], binary_data.X_valid[:, 1], c=binary_data.y_valid)\n",
    "axs[2].scatter(binary_data.X_test[:, 0], binary_data.X_test[:, 1], c=binary_data.y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data! The `binary_data` instance has the following attributes:\n",
    "   - a training set (`X_train, y_train`): to train the model and on which the prediction is based\n",
    "   - a validation set (`X_valid, y_valid`): to select the best __hyper-parameters__ of the model\n",
    "   - a test set (`X_test, y_test`): to evaluate the performance of the model on unseen data\n",
    "\n",
    "### Problem 1: Complete our KNN Classifier - 46 Points + 3 Bonus\n",
    "\n",
    "\n",
    "The KNNClassifier class we're implementing will have similar design to the [KNeighbors classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) from *scikit-learn*:\n",
    "- Initialize the classifier with corresponding parameters (number of neighbors k)\n",
    "- A `fit` method that uses the training data\n",
    "- A `predict` method that returns the predicted labels given data `X`\n",
    "\n",
    "We've written out a lot of the structure for you for consistency across different parts of the assignment and so\n",
    "you can focus on the \"important\" stuff that actually relates to machine learning itself. See the individual questions to complete below this cell.\n",
    "\n",
    "_Note: We have implemented the \"fit\" step using an existing [BallTree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree) structure as discussed in lecture._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "knnc",
     "locked": false,
     "points": 30,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "\n",
    "    def __init__(self, k=5):\n",
    "        \"\"\" Initialize our custom KNN classifier\n",
    "        Args:\n",
    "            k: the number of nearest neighbors to consider for classification\n",
    "        \"\"\"\n",
    "        self._k = k\n",
    "        self._ball_tree = None\n",
    "        self._y = None\n",
    "        self.label_to_index = None\n",
    "        self.index_to_label = None\n",
    "        self.training_most_common = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the model using the provided data\n",
    "        Args:\n",
    "            X:  matrix of shape (num_training_samples, num_features)\n",
    "            y: array of shape (num_training_samples,)\n",
    "        Returns: Fit instance of KNNClassifier (self)\n",
    "        \"\"\"\n",
    "        self._ball_tree = sklearn.neighbors.BallTree(X)  # See documentation of BallTree and how it's used\n",
    "        self._y = y\n",
    "        # Should be used to map the classes to {0,1,..C-1} if needed (C is the number of classes)\n",
    "        # We can assume that the training data contains samples from all the possible classes\n",
    "        classes = np.unique(y)\n",
    "        self.label_to_index = dict(zip(classes, range(classes.shape[0])))\n",
    "        self.index_to_label = dict(zip(range(classes.shape[0]), classes))\n",
    "        \n",
    "        label_values, label_counts = np.unique(y, return_counts=True)\n",
    "        self.training_most_common = label_values[np.argmax(label_counts)]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def sample_label(self, index):\n",
    "        # helper method to get label of sample index in majority_vote\n",
    "        assert index < self._y.shape[0]\n",
    "        return self._y[index]\n",
    "\n",
    "    def majority_vote(self, indices_nearest_k, distances_nearest_k=None):\n",
    "        \"\"\" Given indices of the nearest k neighbors for each point,\n",
    "            report the majority label of those points.\n",
    "            Use the training set's most common label in case of a tie.\n",
    "        Args:\n",
    "            k_nearest_indices: 2-d array of the indices of training neighbors, of shape (M, k)\n",
    "            k_nearest_distances: 2-d array of the corresponding distances of training neighbors, of shape (M, k)\n",
    "        Returns: The majority label for each row of indices, shape (M,)\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 1.2\n",
    "        # TODO: Determine majority for each row of indices_nearest_k\n",
    "        # TODO: if there is a tie, use the most common label from the training set (stored during Fit).\n",
    "        voted_labels = np.empty(indices_nearest_k.shape[0])  # to include\n",
    "        #BEGIN\n",
    "                    \n",
    "        #END\n",
    "        return voted_labels\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Given new data points, classify them according to the training data\n",
    "            provided in self.fit and number of neighbors self.k\n",
    "            - You should use BallTree to get the distances and indices of\n",
    "            the nearest k neighbors\n",
    "        Args:\n",
    "            X: feature vectors (num_samples, num_features)\n",
    "        Returns:\n",
    "            1-D array of predicted classes of shape (num_samples,)\n",
    "        \"\"\"\n",
    "        # Workspace 1.1\n",
    "        # inc distances_nearest_k, indices_nearest_k = None, None  # REPLACE\n",
    "        #BEGIN\n",
    "        #END\n",
    "        return self.majority_vote(indices_nearest_k, distances_nearest_k)\n",
    "\n",
    "    def confusion_matrix(self, X, y):\n",
    "        \"\"\" Generate the confusion matrix for the given data\n",
    "        Args:\n",
    "            X: data matrix, shape (num_samples, num_features)\n",
    "            y: the corresponding correct classes of our set, shape (num_samples,)\n",
    "        Returns: a CxC matrix, where C is the number of classes in our training data\n",
    "        \"\"\"\n",
    "\n",
    "        # The rows of the confusion matrix correspond to the counts from the true labels, the columns to the predictions'\n",
    "        # Workspace 1.3\n",
    "        # TODO: Run classification for the test set X, compare to test answers y, and add counts to matrix\n",
    "        c_matrix = np.zeros((len(self.label_to_index), len(self.label_to_index)))\n",
    "        #BEGIN\n",
    "        #END\n",
    "        return c_matrix\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\" Return the accuracy of the classifier on the data (X, y)\n",
    "        Args:\n",
    "            X: matrix of shape (num_samples, num_features)\n",
    "            y: array of shape (num_samples,)\n",
    "\n",
    "        Returns: accuracy score [float in (0,1)]\n",
    "        \"\"\"\n",
    "        # Workspace 1.4\n",
    "        # TODO: Compute accuracy on X\n",
    "        # inc score = 0\n",
    "        #BEGIN\n",
    "        #END\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Test cell, uncomment to run the tests\n",
    "#%run -i tests knnc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*But professor, this code isn't complete!*\n",
    "\n",
    "\n",
    "\n",
    "**Q1.1** *[5 points]*  Complete the `predict` function to capture the predicted class of a new datapoint based on the result of the `majority_vote` function.\n",
    "\n",
    " - HINT: Use the [BallTree documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree) to determine how to retrieve neighbors from the model.\n",
    "\n",
    "**Q1.2** *[10 points]*  Complete the `majority_vote` function to determine the majority class of a series of neighbors.\n",
    "    If there is a tie, then you should remove the farthest element until the tie is broken.\n",
    "\n",
    " - HINT: Use the [BallTree documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree) to determine what information you have to determine the farthest element.\n",
    "\n",
    "**Q1.3** *[10 points]*  Complete the `confusion_matrix` function to reveal the results of classification\n",
    "\n",
    "**Q1.4** *[5 points]*  Complete the `accuracy` function to get accuracy of the classifier based on a given test data\n",
    "\n",
    "Below, we'll be using our KNNClassifier (sent in as \"model\") to show how we would predict any points in space given the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell is to show the decision surface of the classifier\n",
    "# You can change k to visualize KNN behavior\n",
    "knn = KNNClassifier(2).fit(binary_data.X_train, binary_data.y_train)\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.set_figheight(6), fig.set_figwidth(18)\n",
    "helpers.show_decision_surface(knn, binary_data.X_train, binary_data.y_train, axs[0])\n",
    "helpers.show_decision_surface(knn, binary_data.X_valid, binary_data.y_valid, axs[1])\n",
    "helpers.show_decision_surface(knn, binary_data.X_test, binary_data.y_test, axs[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Q1.5** *[10 points]*  For each k in the range [1,32], fit a KNNClassifier on the training set and plot the accuracies on training and validation data versus k.\n",
    "\n",
    "What's the value of k that yields the best accuracy on the training set? on the validation set? Which one should we choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "knneval",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Workspace 1.5.a\n",
    "#TODO: Try different Ks\n",
    "ks = list(range(1, 32))\n",
    "accuracies_train = []\n",
    "accuracies_valid = []\n",
    "for k in ks:\n",
    "    #BEGIN\n",
    "    \n",
    "    #END\n",
    "plt.plot(ks, accuracies_valid, label=\"valid\")\n",
    "plt.plot(ks, accuracies_train, label=\"train\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### # Workspace 1.5.b\n",
    "% Write up: best k for training and validation and which one should we choose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Q1.6** *[6 points]*  Report the accuracy and the confusion matrix on the test set using the value of k chosen in 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q16",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Workspace 1.6\n",
    "# TODO: compute and print the accuracy on the test set using k from 1.5\n",
    "# inc best_k = None\n",
    "#BEGIN\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus (for the avid machine learner) (3 Points)**\n",
    "\n",
    "1.7.a [1 point] A [__consistent classifier__](https://proceedings.neurips.cc/paper/1996/file/7bb060764a818184ebb1cc0d43d382aa-Paper.pdf)\n",
    "on the training data is defined as a classifier that reaches 100% accuracy on the training set. For which values of k is KNNClassifier Consistent?\n",
    "\n",
    "1.7.b [2 points] Edit your `KNNClassifier` so that it's consistent for all $k$ (we made sure that the changes do not affect the sanity checks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6353ee34537056ea",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "#### Write-up for the bonus\n",
    "**Workspace 1.7.a**\n",
    "\n",
    "% for which k in KNNClassifier consistent\n",
    "\n",
    "#BEGIN\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "OK - now we've demonstrated that our KNN classifier works, let's think about our problem space!\n",
    "\n",
    "## Problem 2: Improving KNN on Digits dataset - 24 Points\n",
    "\n",
    "It's a pretty common problem - just imagine working at the post office, and you're handed a hand-written check,\n",
    "and you have to identify exactly what it says.\n",
    "Did they pay 500 or 600 dollars? Is the letter going to 80309 (campus) or 30309 (Atlanta)?\n",
    "\n",
    "**Q2.1** *[7 points]* Complete `prepare_data` by reporting the number of examples in different partitions of the digits dataset and the number of pixels in the images. You also have to reshape `X_train, X_valid, X_test` arrays as matrices for our KNNClassifier to use them.\n",
    "\n",
    "Note: the question is simple and straight-forward, there is no trick here,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "digit_data = data.DigitData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7fbb8b5204b8b885",
     "locked": false,
     "points": 7,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(dataset: data.Dataset):\n",
    "    \"\"\"\n",
    "    Report information about the dataset using the print() function and reshape X_? to 2-d arrays\n",
    "    \"\"\"\n",
    "    # Workspace 2.1\n",
    "    #BEGIN \n",
    "    #TODO: Create printouts for reporting the size of each set and the size of each datapoint\n",
    "    #END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prepare_data(dataset=digit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(8, 24))\n",
    "helpers.view_digit(digit_index=1,dataset = digit_data, partition=\"test\", ax=ax[0])\n",
    "helpers.view_digit(digit_index=5,dataset = digit_data, partition=\"test\", ax=ax[1])\n",
    "helpers.view_digit(digit_index=5,dataset = digit_data, partition=\"valid\", ax=ax[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to automatically perform the model selection from 1.5, 1.6\n",
    "\n",
    "\n",
    "**Q2.2** *[10 points]* complete the `evaluate` to perform the same evaluation we did in 1.5:\n",
    " - For k in range (1, 20):\n",
    "    - initialize the classifier for k  and train in on the training set\n",
    "    - Compute the accuracy on the validation set and save it\n",
    " - Choose k with the best accuracy on the validation set\n",
    " - Report the accuracy and the confusion matrix on the test set (note we use `display_confusion` for a cleaner output of your model's `confustion_matrix` result)\n",
    "\n",
    " - Hint: In working on this problem, you may discover your KNNClassifier does not successfully generalize to multi-class cases. You may have to edit your answers to Problem 1!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-343ea2479f6f3481",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(ks_range,dataset: data.Dataset, KNNClass=KNNClassifier):\n",
    "    \"\"\" Evaluate KNNClassifier of Dataset for k in ks_range by printing the accuracy and confusion matrix of the best k\n",
    "    Args:\n",
    "        ks_range: range of k values\n",
    "        dataset: dataset\n",
    "    \"\"\"\n",
    "    # Workspace 2.2\n",
    "    # inc best_valid_k = None\n",
    "    # inc confusion_matrix = None\n",
    "    # inc accuracy = 0\n",
    "    ks = ks_range\n",
    "    accuracies_valid = []\n",
    "    confusion_matrix = []\n",
    "    for k in ks:\n",
    "        print(k, end=\"\\r\")\n",
    "        #BEGIN\n",
    "    #END\n",
    "    print(\"best k:\", best_valid_k)\n",
    "    print(\"Accuracy on test set:\", accuracy)\n",
    "    helpers.display_confusion(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the next cell, we run the evaluation on k in the range ${2...9}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "evaluate(range(2, 10), digit_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Q2.3** [7 points] Determine which classes are most often confused (from our confusion matrix above).\n",
    "Display examples (at least 3) of **misclassified test instances** (yes, actual samples that were classified incorrectly by your final classifier) and explain why the KNNClassifier might have missed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4466a5cfc18f8096",
     "locked": false,
     "points": 7,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Workspace 2.3.a\n",
    "#TODO: Print out problem class images\n",
    "helpers.view_digit(digit_index=1,dataset = digit_data, partition=\"test\")\n",
    "#BEGIN\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b653a6df29b47bb3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Workspace 2.3.b**\n",
    "\n",
    "TODO: Write description of mis-classification\n",
    "\n",
    "#BEGIN\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Problem 3 - KNNRegressor [30 points]\n",
    "***\n",
    "Let's consider a different problem - Regression. Regression deals with continuous target values (labels).\n",
    "For this problem we'll use the [California Housing Prices dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "housing_data = data.HousingPrices()\n",
    "print(\"Features matrix shape:\", housing_data.X_train.shape)\n",
    "plt.title(\"Training data prices distribution\")\n",
    "plt.hist(housing_data.y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The KNNRegressor(k) predicts the target value by uniformly averaging the target values of the k-nearest neighbors from the training data.\n",
    "\n",
    "**Q3.1** [10 points]  Complete `average_vote` function\n",
    "\n",
    "**Q3.2** [10 points]  Complete the `mse` function. Do not use sklearn built-in functions (only numpy functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-92caebe1bf62d72f",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class KNNRegressor(KNNClassifier):\n",
    "    def average_vote(self, indices_nearest_k, distances_nearest_k=None):\n",
    "        \"\"\" Given indices of the nearest k neighbors for each point, report the average y value of those points.\n",
    "        Args:\n",
    "            indices_nearest_k: np.array containing the indices of training neighbors, of shape (M, k)\n",
    "            distances_nearest_k: np.array containing the corresponding distances of training neighbors,\n",
    "            of shape (M, k), for the bonus\n",
    "        Returns:The majority label for each row of indices, shape (M,)\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 3.1\n",
    "        # TODO: Calculate the average y value for each set of neighbors and store in voted_labels\n",
    "        voted_labels = np.empty(indices_nearest_k.shape[0])  # to include\n",
    "        #BEGIN\n",
    "        #END\n",
    "        return voted_labels\n",
    "\n",
    "    def predict(self, X):\n",
    "        distances_nearest_k, indices_nearest_k = self._ball_tree.query(X, k=self._k)\n",
    "        return self.average_vote(indices_nearest_k, distances_nearest_k)\n",
    "\n",
    "    def mse(self, X, y):\n",
    "        \"\"\"\n",
    "         Return the Mean Squared Error of the classifier\n",
    "        Args:\n",
    "            X: np.array of shape (m, number_features)\n",
    "            y: np.array of shape (m,)\n",
    "        Returns:MSE  (y, y_hat)\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 3.2\n",
    "        # TODO: Compute mse across all samples\n",
    "        score = 0\n",
    "        #BEGIN\n",
    "        #END\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Q3.3** [10 points] Similar to 1.5, for each k in the range [1,32], fit a KNNRegressor on the training set and plot the Mean Squared Errors on training and validation data versus k.\n",
    "\n",
    "What's the value of k that yields the best mse on the training set? on the validation set? Which k should we choose?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ad46892a92a1e7db",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Workspace 3.3.a\n",
    "#TODO: Try different Ks\n",
    "ks = list(range(1, 32))\n",
    "accuracies_train = []\n",
    "accuracies_valid = []\n",
    "for k in ks:\n",
    "    #BEGIN\n",
    "    #END\n",
    "plt.plot(ks, accuracies_valid, label=\"valid\")\n",
    "plt.plot(ks, accuracies_train, label=\"train\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Workspace 3.3 writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Problem 4 : Improving KNN on Binary and MNIST using WeightedKNN (10 Bonus pts)\n",
    "Complete the `WeightedKNNClassifier` class to perform the weighted KNN classification.\n",
    "The Weighted KNN classifier assigns weights to the nearest-neighbor training examples proportional to\n",
    " the inverse-distance from the training example to the query point.\n",
    "\n",
    "Classification is performed by summing the weights associated with each class and predicting the class with the highest weighted-majority vote.\n",
    " Mathematically, we might describe the weighted-vote for a class $c$ as\n",
    "\n",
    "\\begin{align}\n",
    "\\textrm{Weighted-Vote}(c)(x) = \\sum_{i \\in {\\cal N}_K(x)} I(y_i \\in c) \\times \\frac{1}{\\|{\\bf x}_i - {\\bf x}\\|}\n",
    "\\end{align}\n",
    "\n",
    "where ${\\cal N}_K(x)$ is the set of the nearest $k$ neighbors to $x$.\n",
    "\n",
    "**Q4.1** *[5 points]* Complete `weighted_vote`: it's certainly possible that a query point could be distance $0$ away from some training example. If this happens your implementation should handle it and return the appropriate class label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-24fbf40756e38ede",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class WeightedKNNClassifier(KNNClassifier):\n",
    "\n",
    "    def weighted_vote(self, indices_nearest_k, distances_nearest_k):\n",
    "        \"\"\" Given indices of nearest neighbors in training set, return the majority label.\n",
    "        Break ties by using the most common label found in the training data.\n",
    "        Args:\n",
    "            indices_nearest_k: The indices of the K nearest neighbors in self.X_train\n",
    "            distances_nearest_k: Corresponding distances from query point to K nearest neighbors.\n",
    "        Returns: predicted labels\n",
    "        \"\"\"\n",
    "\n",
    "        # Workspace 4.1\n",
    "        \n",
    "        labels = []  #You can replace\n",
    "        #BEGIN \n",
    "        #END\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Given an np.array of query points, return y_hat, an np.array of predictions\n",
    "        Args:\n",
    "            X: an (m x p) dimension np.array of points to predict labels for\n",
    "        Returns: predicted labels\n",
    "        \"\"\"\n",
    "\n",
    "        distances_nearest_k, indices_nearest_k = self._ball_tree.query(X, k=self._k)\n",
    "        labels = self.weighted_vote(indices_nearest_k, distances_nearest_k)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "#%run -i tests weightedknnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Example on how to use the show_decision_surface\n",
    "knn = WeightedKNNClassifier(3).fit(binary_data.X_train, binary_data.y_train)\n",
    "helpers.show_decision_surface(knn, binary_data.X, binary_data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Q4.2** *[3 points]*Evaluate `WeightedKNNClassifier` on the binary data (use `evaluate` function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4bfac572c191c9ce",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Workspace 4.2\n",
    "#BEGIN\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Workspace 4.2.b\n",
    "% Write up: Which classifier does better on the binary data? What is better / worse in each?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.3** *[2 points]* Compare `WeightedKNNClassifier` to `KNNClassifier` on the digits' data using `Numbers` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3587ad11c6895946",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Workspace 4.3.a\n",
    "#BEGIN\n",
    "#END\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-37731580a19cb922",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "### # Workspace 4.3.b\n",
    "% Write up: Which classifier does better on the digits data? What is better / worse in each?\n",
    "\n",
    "#BEGIN\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
